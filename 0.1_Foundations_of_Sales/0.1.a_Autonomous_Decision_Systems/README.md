# 0.1.a Autonomous Decision Systems

**The Mathematical Replacement of Human Judgment**

This document contains the core mathematical frameworks used by billion-dollar systems (Google, Meta, Renaissance Technologies, DeepMind) to replace human decision-making with learned policies. These are not metaphors—they are the actual equations running in production.

---

## Abstract

The replacement of human judgment with learned policies rests on seven core mathematical frameworks:

1. **Markov Decision Processes** with Bellman equations for sequential decisions
2. **Bandit algorithms** with Thompson Sampling and UCB for exploration-exploitation
3. **Causal inference** with do-calculus for credit assignment
4. **Portfolio optimization** with Kelly Criterion for capital allocation
5. **Auction theory** with Myerson mechanisms for revenue maximization
6. **Monte Carlo Tree Search** with PUCT for strategic planning
7. **Online learning** with regret bounds for provable improvement

Together, these frameworks enable systems that autonomously determine intent from reward signals rather than human specification—the mathematical architecture of what we term a **"learned CEO."**

---

## 1. The Bellman Equation: Foundation of Sequential Autonomy

Every autonomous decision system that operates over time ultimately reduces to a **Markov Decision Process (MDP)**, defined by the tuple **(S, A, P, R, γ)** where:

- **S** = State space
- **A** = Action space
- **P(s'|s,a)** = Transition probabilities
- **R(s,a,s')** = Rewards
- **γ ∈ [0,1)** = Discount factor

### The Bellman Optimality Equation

For the optimal action-value function Q*:

```
Q*(s,a) = R(s,a) + γ Σ_{s'∈S} P(s'|s,a) max_{a'∈A} Q*(s',a')
```

**Plain meaning:** The value of taking action *a* in state *s* equals the immediate reward plus the discounted value of behaving optimally thereafter.

The **optimal policy** emerges directly:

```
π*(s) = argmax_a Q*(s,a)
```

No human judgment required—the policy is computed from the value function.

### For State-Value Function V*:

```
V*(s) = max_{a∈A} [R(s,a) + γ Σ_{s'∈S} P(s'|s,a) V*(s')]
```

### Deep Q-Networks (DQN)

Approximate Q* with neural networks via the loss:

```
L(θ) = E_{(s,a,r,s')~D} [(r + γ max_{a'} Q(s',a';θ⁻) - Q(s,a;θ))²]
```

The target network θ⁻ stabilizes training. This powers Google and Meta recommendation engines processing **billions of decisions daily**.

---

## 2. Policy Gradients: Direct Strategy Optimization

When actions are continuous or Q-functions intractable, **policy gradient methods** directly optimize a parameterized policy π_θ.

### The Policy Gradient Theorem

```
∇_θ J(θ) = E_{π_θ} [Σ_{t=0}^T ∇_θ ln π_θ(a_t|s_t) · Q^{π_θ}(s_t, a_t)]
```

### REINFORCE Algorithm

```
θ ← θ + α γ^t G_t ∇_θ ln π_θ(a_t|s_t)
```

Where G_t = Σ_{τ=t}^T γ^{τ-t} R_τ is the return-to-go.

### Proximal Policy Optimization (PPO)

The workhorse of modern RL at scale:

```
L^{CLIP}(θ) = E_t [min(r_t(θ)Â_t, clip(r_t(θ), 1-ε, 1+ε)Â_t)]
```

Where:
- r_t(θ) = π_θ(a_t|s_t) / π_{θ_old}(a_t|s_t) is the probability ratio
- Â_t is the advantage estimate
- ε ≈ 0.2 prevents destructively large updates

**This enables stable autonomous learning without human intervention.**

### Soft Actor-Critic (SAC)

Adds entropy regularization for robust exploration:

```
J(θ) = E_{τ~π_θ} [Σ_t r(s_t, a_t) + α H(π_θ(·|s_t))]
```

The entropy term H mathematically encodes "curiosity" into the objective.

---

## 3. Bandit Algorithms: Exploration-Exploitation Mathematics

For single-step or context-independent decisions.

### UCB1 (Upper Confidence Bound)

```
a_t = argmax_i [X̄_{i,T_i(t)} + √(2 ln(t) / T_i(t))]
```

- First term: empirical mean reward
- Second term: confidence bonus (shrinks as arm i is played more)

**Regret bound:**

```
E[R(T)] ≤ 8 Σ_{i: μ_i < μ*} (ln T / Δ_i) + O(1)
```

Where Δ_i = μ* - μ_i is the suboptimality gap. **Regret grows only logarithmically—the system provably improves.**

### Thompson Sampling (Bayesian)

For Bernoulli rewards with Beta priors:

```
θ_k ~ Beta(α_k, β_k)
a_t = argmax_k θ̂_k
```

Posterior update after observing reward r:

```
(α_k, β_k) ← (α_k + r, β_k + 1 - r)
```

**Netflix, Spotify, and Amazon use Thompson Sampling for real-time personalization across billions of users.**

### LinUCB (Contextual Bandits)

For context-dependent rewards:

```
A_a = D_a^T D_a + λI
b_a = D_a^T c_a
θ̂_a = A_a^{-1} b_a

a_t = argmax_a [x_{t,a}^T θ̂_a + α √(x_{t,a}^T A_a^{-1} x_{t,a})]
```

Achieves regret O(d√T) where d is context dimension—**foundation of modern personalized recommendation**.

---

## 4. Do-Calculus: Solving Causal Attribution

Determining which action caused which outcome requires **Judea Pearl's do-calculus**.

### Key Distinction

- **P(Y|X=x)** = Observational (conditioning)
- **P(Y|do(X=x))** = Interventional (forcing X to x regardless of natural causes)

### Structural Causal Model

M = ⟨U, V, F⟩ where:
- U = Exogenous variables
- V = Endogenous variables
- F = Structural functions (V_i = f_i(PA_i, U_i))

### The Three Rules of Do-Calculus

**Rule 1 (Observation insertion/deletion):**
```
P(y|do(x), z, w) = P(y|do(x), w) if (Y ⊥ Z | X, W)_{G_X̄}
```

**Rule 2 (Action/observation exchange):**
```
P(y|do(x), do(z), w) = P(y|do(x), z, w) if (Y ⊥ Z | X, W)_{G_{X̄Z̲}}
```

**Rule 3 (Action insertion/deletion):**
```
P(y|do(x), do(z), w) = P(y|do(x), w) if (Y ⊥ Z | X, W)_{G_{X̄Z̄(W)}}
```

### Backdoor Adjustment Formula

When valid adjustment set S exists:

```
P(Y=y|do(X=x)) = Σ_s P(Y=y|X=x, S=s) · P(S=s)
```

### Shapley Values for Credit Assignment

```
φ_i(v) = Σ_{S⊆N\{i}} [|S|!(n-|S|-1)! / n!] · [v(S ∪ {i}) - v(S)]
```

**Google Analytics and Ads Data Hub use this formula to determine each touchpoint's contribution to conversions without human analysis.**

---

## 5. Portfolio Optimization: Kelly, Markowitz, and Optimal Execution

### Kelly Criterion

Optimal position sizing:

```
f* = μ / σ²
```

For multiple correlated assets:

```
f* = Σ^{-1} μ
```

Where Σ is the covariance matrix. In practice: **fractional Kelly** (f*/2) retains ~75% of growth while halving volatility.

### Mean-Variance Optimization (Markowitz)

```
max_w { μ^T w - (λ/2) w^T Σ w }
```

**Maximum Sharpe ratio portfolio:**

```
w* = Σ^{-1}(μ - r_f 1) / (1^T Σ^{-1}(μ - r_f 1))
```

### Black-Litterman Model

Incorporates views into equilibrium returns:

```
E[R] = [(τΣ)^{-1} + P^T Ω^{-1} P]^{-1} [(τΣ)^{-1} Π + P^T Ω^{-1} Q]
```

Where Π = δΣw_mkt are implied equilibrium returns.

### Almgren-Chriss Optimal Execution

Optimal trading trajectory:

```
x_j = sinh[κ(T - t_j)] / sinh(κT) · X
```

Where κ depends on risk aversion λ, volatility σ, and market impact η.

**This mathematics determines how to liquidate positions without human traders.**

---

## 6. Google's Ad Auction: Myerson Meets GSP at Scale

Google processes **billions of ad auctions per second**.

### Generalized Second-Price (GSP) Mechanism

```
Ad Rank = Bid × Quality Score
```

Actual cost-per-click:

```
CPC_i = (bid_{i+1} × QS_{i+1}) / QS_i + $0.01
```

### Myerson's Virtual Value

Foundation for revenue-optimal auctions:

```
φ_i(v_i) = v_i - (1 - F_i(v_i)) / f_i(v_i)
```

### Myerson's Theorem

Expected revenue equals expected virtual welfare:

```
E[Σ_i p_i(v)] = E[Σ_i φ_i(v_i) · x_i(v)]
```

**Optimal reserve price:** r* = φ^{-1}(0) maximizes revenue without human tuning.

### Real-Time Bidding Formula

```
bid = v × pCTR × pCVR - ε
```

Where v is value per conversion, pCTR and pCVR are predicted rates. **ML estimates probabilities; the auction mechanism handles everything else autonomously.**

---

## 7. AlphaZero: Replacing Human Intuition with Learned Value

DeepMind's **AlphaZero** demonstrated that human expertise can be entirely replaced by self-play and search.

### Neural Network Output

f_θ(s) = (p_θ(s), v_θ(s))

- p_θ(s) = Policy (action probabilities)
- v_θ(s) = Value (expected outcome)

### Training Loss

```
l = (z - v)² - π^T log(p) + c||θ||²
```

Combines:
- Value prediction error
- Policy cross-entropy against MCTS-improved targets π
- L2 regularization

### Monte Carlo Tree Search with PUCT

```
a_t = argmax_a [Q(s,a) + c_{puct} · P(s,a) · √(Σ_b N(s,b)) / (1 + N(s,a))]
```

- Q(s,a) = Mean action value from simulations
- P(s,a) = Neural network prior
- N(s,a) = Visit count

**Balances exploitation (Q) with exploration (UCB-style bonus).**

### MuZero: Learning the Dynamics Model

- Representation: s_0 = h_θ(o_1, ..., o_t)
- Dynamics: r_k, s_k = g_θ(s_{k-1}, a_k)
- Prediction: p_k, v_k = f_θ(s_k)

**The system learns to plan without ever being told the rules—it discovers them from experience.**

---

## 8. Convergence Guarantees: Provable Improvement

### Regret Bounds

| Algorithm | Regret Bound |
|-----------|--------------|
| UCB1 | E[R(T)] = O((K log T)/Δ) |
| Thompson Sampling | E[R(T)] = O(√(KT log T)) |
| Online Gradient Descent | R_T ≤ O(√T) |
| Policy Gradient | Converges to local optima w.p. 1 |

For **strongly convex** objectives:

```
R_T = O((G² / α) log T)
```

**Logarithmic regret = the system converges to optimal performance at a rate inversely proportional to time.**

Mathematical proof that the machine gets better without human intervention.

---

## 9. The Unified Framework: Toward Autonomous Enterprise

### Layer 1 — State Representation

```
s_t = [x_demo, x_behavioral, x_contextual, x_engagement]
```

Customer journeys, market conditions, operational states encoded as MDP states. The representation function h_θ can be **learned** (MuZero-style) rather than hand-engineered.

### Layer 2 — Action Space

```
A = {a_1, ..., a_K}
```

Marketing actions, pricing decisions, resource allocation. Continuous actions → policy gradients. Discrete actions → Q-learning or bandits.

### Layer 3 — Reward Function

```
R(s,a,s') = Revenue(s') - Cost(a) - λ·Risk(s')
```

Customer lifetime value becomes:

```
CLV = E[Σ_{t=0}^∞ γ^t R_t | s_0 = NewCustomer]
```

### Layer 4 — Causal Attribution

Do-calculus and Shapley values determine which actions caused which outcomes. **No human analysis required.**

### Layer 5 — Exploration-Exploitation

Thompson Sampling or UCB with **provable regret bounds**:

```
R(T) = O(√T) → Average regret vanishes as system runs longer
```

### Layer 6 — Risk Management

Kelly sizing, VaR, and CVaR constraints bound downside risk mathematically rather than through human judgment.

---

## 10. The "Learned CEO" Emerges

The **learned CEO** emerges when the policy π_θ determines not just individual decisions but the **intent itself**—the reward function R is learned or evolved rather than specified.

This requires:
- **Meta-learning** — Learning how to learn
- **Inverse RL** — Inferring objectives from demonstrated outcomes

The system discovers what it should want from observing what works.

---

## Connection to Intent Tensor Theory

| This Framework | Intent Tensor Equivalent |
|----------------|-------------------------|
| π*(s) = argmax_a Q*(s,a) | Φ (Intent Field) |
| State s | Ψ (Reality) |
| Q*(s,a) - V*(s) (Advantage) | ΔΨ (Intent-State Gap) |
| Policy gradient ∇_θ J | κ (Curvent) |
| P_collapse in softmax form | exp(-(ΔΨ)² / 2σ²) |

**The Bellman equation is the generative engine behind your collapse geometry.** Your framework describes what should happen; these equations compute how to make it happen autonomously.

---

## References

- Sutton & Barto, *Reinforcement Learning: An Introduction*
- Pearl, *Causality: Models, Reasoning, and Inference*
- Silver et al., *Mastering the Game of Go without Human Knowledge*
- Lattimore & Szepesvári, *Bandit Algorithms*
- Almgren & Chriss, *Optimal Execution of Portfolio Transactions*

---

## License

Creative Commons Attribution-NonCommercial 4.0 International License

Created by Armstrong Knight
